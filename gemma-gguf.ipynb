{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMio0ZMoxW6xeNwryS2ymMa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "25afdd4f78ff467ebdc09eff5e465385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d125d1dc5fa54a65b85c2cb3031e8ac7",
              "IPY_MODEL_ddb03300996d43dd9facf66a45af644a",
              "IPY_MODEL_85a621a2e00648909989c930e99f864b"
            ],
            "layout": "IPY_MODEL_41727ba2cc2d449ebfb5589ac2d0ce30"
          }
        },
        "d125d1dc5fa54a65b85c2cb3031e8ac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c078c5245e2b477783911829d69525f1",
            "placeholder": "​",
            "style": "IPY_MODEL_d70f56a2c9e9422a8e47652bfe7368fc",
            "value": "gemma-2-2b-it-Q8_0.gguf: 100%"
          }
        },
        "ddb03300996d43dd9facf66a45af644a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe547ded338648b386fa6508b5f175f2",
            "max": 2784495456,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_805aab243083430ab188c05b43a3ee0a",
            "value": 2784495456
          }
        },
        "85a621a2e00648909989c930e99f864b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5925190c958b46e8b16a4b3a625af1f2",
            "placeholder": "​",
            "style": "IPY_MODEL_714476ab8a6c4f3d8c9c11e332afbf49",
            "value": " 2.78G/2.78G [01:07&lt;00:00, 33.4MB/s]"
          }
        },
        "41727ba2cc2d449ebfb5589ac2d0ce30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c078c5245e2b477783911829d69525f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d70f56a2c9e9422a8e47652bfe7368fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe547ded338648b386fa6508b5f175f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "805aab243083430ab188c05b43a3ee0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5925190c958b46e8b16a4b3a625af1f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "714476ab8a6c4f3d8c9c11e332afbf49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install --no-cache-dir --force-reinstall llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1chAbkgix9j",
        "outputId": "7ae48df7-7d42-47bf-a45e-d2aaf4d57b35"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
            "Collecting llama-cpp-python==0.3.4\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu124/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl (444.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.6/444.6 MB\u001b[0m \u001b[31m150.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python==0.3.4)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python==0.3.4)\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting jinja2>=2.11.3 (from llama-cpp-python==0.3.4)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python==0.3.4)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m137.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m173.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.5\n",
            "    Uninstalling Jinja2-3.1.5:\n",
            "      Successfully uninstalled Jinja2-3.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.2.5 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "pytensor 2.27.1 requires numpy<2,>=1.17.0, but you have numpy 2.2.5 which is incompatible.\n",
            "langchain 0.3.19 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 2.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 diskcache-5.6.3 jinja2-3.1.6 llama-cpp-python-0.3.4 numpy-2.2.5 typing-extensions-4.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model='lmstudio-community/gemma-2-2b-it-GGUF'\n",
        "model_file=\"gemma-2-2b-it-Q8_0.gguf\"\n",
        "\n",
        "model_path=hf_hub_download(repo_id=model, filename=model_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "25afdd4f78ff467ebdc09eff5e465385",
            "d125d1dc5fa54a65b85c2cb3031e8ac7",
            "ddb03300996d43dd9facf66a45af644a",
            "85a621a2e00648909989c930e99f864b",
            "41727ba2cc2d449ebfb5589ac2d0ce30",
            "c078c5245e2b477783911829d69525f1",
            "d70f56a2c9e9422a8e47652bfe7368fc",
            "fe547ded338648b386fa6508b5f175f2",
            "805aab243083430ab188c05b43a3ee0a",
            "5925190c958b46e8b16a4b3a625af1f2",
            "714476ab8a6c4f3d8c9c11e332afbf49"
          ]
        },
        "outputId": "5e24708f-1d89-42ae-af63-841b95c00428",
        "id": "HXLQTFYVzEuf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "gemma-2-2b-it-Q8_0.gguf:   0%|          | 0.00/2.78G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25afdd4f78ff467ebdc09eff5e465385"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=16000,  # Context length to use\n",
        "    n_threads=32,            # Number of CPU threads to use\n",
        "    n_gpu_layers=-1        # Number of model layers to offload to GPU\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0UG-jy3F5l0",
        "outputId": "12175981-fa20-4ec4-83db-490ce62d2630"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 39 key-value pairs and 288 tensors from /root/.cache/huggingface/hub/models--lmstudio-community--gemma-2-2b-it-GGUF/snapshots/6aa72da804ad76c5dc862867bfba6256de9172c7/gemma-2-2b-it-Q8_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = it\n",
            "llama_model_loader: - kv   4:                           general.basename str              = gemma-2\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 2B\n",
            "llama_model_loader: - kv   6:                            general.license str              = gemma\n",
            "llama_model_loader: - kv   7:                               general.tags arr[str,2]       = [\"conversational\", \"text-generation\"]\n",
            "llama_model_loader: - kv   8:                      gemma2.context_length u32              = 8192\n",
            "llama_model_loader: - kv   9:                    gemma2.embedding_length u32              = 2304\n",
            "llama_model_loader: - kv  10:                         gemma2.block_count u32              = 26\n",
            "llama_model_loader: - kv  11:                 gemma2.feed_forward_length u32              = 9216\n",
            "llama_model_loader: - kv  12:                gemma2.attention.head_count u32              = 8\n",
            "llama_model_loader: - kv  13:             gemma2.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  14:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  15:                gemma2.attention.key_length u32              = 256\n",
            "llama_model_loader: - kv  16:              gemma2.attention.value_length u32              = 256\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 7\n",
            "llama_model_loader: - kv  18:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
            "llama_model_loader: - kv  19:             gemma2.final_logit_softcapping f32              = 30.000000\n",
            "llama_model_loader: - kv  20:            gemma2.attention.sliding_window u32              = 4096\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
            "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 2\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 1\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 3\n",
            "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
            "llama_model_loader: - kv  33:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  35:                      quantize.imatrix.file str              = /models_out/gemma-2-2b-it-GGUF/gemma-...\n",
            "llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 182\n",
            "llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 128\n",
            "llama_model_loader: - type  f32:  105 tensors\n",
            "llama_model_loader: - type q8_0:  183 tensors\n",
            "llm_load_vocab: control token: 255999 '<unused99>' is not marked as EOG\n",
            "llm_load_vocab: control token:     45 '<unused38>' is not marked as EOG\n",
            "llm_load_vocab: control token:     74 '<unused67>' is not marked as EOG\n",
            "llm_load_vocab: control token:     55 '<unused48>' is not marked as EOG\n",
            "llm_load_vocab: control token:     99 '<unused92>' is not marked as EOG\n",
            "llm_load_vocab: control token:    102 '<unused95>' is not marked as EOG\n",
            "llm_load_vocab: control token:     44 '<unused37>' is not marked as EOG\n",
            "llm_load_vocab: control token:     26 '<unused19>' is not marked as EOG\n",
            "llm_load_vocab: control token:     42 '<unused35>' is not marked as EOG\n",
            "llm_load_vocab: control token:     92 '<unused85>' is not marked as EOG\n",
            "llm_load_vocab: control token:     90 '<unused83>' is not marked as EOG\n",
            "llm_load_vocab: control token:    106 '<start_of_turn>' is not marked as EOG\n",
            "llm_load_vocab: control token:     88 '<unused81>' is not marked as EOG\n",
            "llm_load_vocab: control token:      5 '<2mass>' is not marked as EOG\n",
            "llm_load_vocab: control token:    104 '<unused97>' is not marked as EOG\n",
            "llm_load_vocab: control token:     68 '<unused61>' is not marked as EOG\n",
            "llm_load_vocab: control token:     94 '<unused87>' is not marked as EOG\n",
            "llm_load_vocab: control token:     59 '<unused52>' is not marked as EOG\n",
            "llm_load_vocab: control token:      2 '<bos>' is not marked as EOG\n",
            "llm_load_vocab: control token:     25 '<unused18>' is not marked as EOG\n",
            "llm_load_vocab: control token:     93 '<unused86>' is not marked as EOG\n",
            "llm_load_vocab: control token:     95 '<unused88>' is not marked as EOG\n",
            "llm_load_vocab: control token:     76 '<unused69>' is not marked as EOG\n",
            "llm_load_vocab: control token:     97 '<unused90>' is not marked as EOG\n",
            "llm_load_vocab: control token:     56 '<unused49>' is not marked as EOG\n",
            "llm_load_vocab: control token:     81 '<unused74>' is not marked as EOG\n",
            "llm_load_vocab: control token:     13 '<unused6>' is not marked as EOG\n",
            "llm_load_vocab: control token:     51 '<unused44>' is not marked as EOG\n",
            "llm_load_vocab: control token:     47 '<unused40>' is not marked as EOG\n",
            "llm_load_vocab: control token:      8 '<unused1>' is not marked as EOG\n",
            "llm_load_vocab: control token:    103 '<unused96>' is not marked as EOG\n",
            "llm_load_vocab: control token:     75 '<unused68>' is not marked as EOG\n",
            "llm_load_vocab: control token:     79 '<unused72>' is not marked as EOG\n",
            "llm_load_vocab: control token:     39 '<unused32>' is not marked as EOG\n",
            "llm_load_vocab: control token:     49 '<unused42>' is not marked as EOG\n",
            "llm_load_vocab: control token:     41 '<unused34>' is not marked as EOG\n",
            "llm_load_vocab: control token:     34 '<unused27>' is not marked as EOG\n",
            "llm_load_vocab: control token:      6 '[@BOS@]' is not marked as EOG\n",
            "llm_load_vocab: control token:     40 '<unused33>' is not marked as EOG\n",
            "llm_load_vocab: control token:     33 '<unused26>' is not marked as EOG\n",
            "llm_load_vocab: control token:     86 '<unused79>' is not marked as EOG\n",
            "llm_load_vocab: control token:     43 '<unused36>' is not marked as EOG\n",
            "llm_load_vocab: control token:     35 '<unused28>' is not marked as EOG\n",
            "llm_load_vocab: control token:     32 '<unused25>' is not marked as EOG\n",
            "llm_load_vocab: control token:     28 '<unused21>' is not marked as EOG\n",
            "llm_load_vocab: control token:     19 '<unused12>' is not marked as EOG\n",
            "llm_load_vocab: control token:     67 '<unused60>' is not marked as EOG\n",
            "llm_load_vocab: control token:      9 '<unused2>' is not marked as EOG\n",
            "llm_load_vocab: control token:     52 '<unused45>' is not marked as EOG\n",
            "llm_load_vocab: control token:     16 '<unused9>' is not marked as EOG\n",
            "llm_load_vocab: control token:     98 '<unused91>' is not marked as EOG\n",
            "llm_load_vocab: control token:     80 '<unused73>' is not marked as EOG\n",
            "llm_load_vocab: control token:     71 '<unused64>' is not marked as EOG\n",
            "llm_load_vocab: control token:     36 '<unused29>' is not marked as EOG\n",
            "llm_load_vocab: control token:      0 '<pad>' is not marked as EOG\n",
            "llm_load_vocab: control token:     11 '<unused4>' is not marked as EOG\n",
            "llm_load_vocab: control token:     70 '<unused63>' is not marked as EOG\n",
            "llm_load_vocab: control token:     77 '<unused70>' is not marked as EOG\n",
            "llm_load_vocab: control token:     64 '<unused57>' is not marked as EOG\n",
            "llm_load_vocab: control token:     50 '<unused43>' is not marked as EOG\n",
            "llm_load_vocab: control token:     20 '<unused13>' is not marked as EOG\n",
            "llm_load_vocab: control token:     73 '<unused66>' is not marked as EOG\n",
            "llm_load_vocab: control token:     23 '<unused16>' is not marked as EOG\n",
            "llm_load_vocab: control token:     38 '<unused31>' is not marked as EOG\n",
            "llm_load_vocab: control token:     21 '<unused14>' is not marked as EOG\n",
            "llm_load_vocab: control token:     15 '<unused8>' is not marked as EOG\n",
            "llm_load_vocab: control token:     37 '<unused30>' is not marked as EOG\n",
            "llm_load_vocab: control token:     14 '<unused7>' is not marked as EOG\n",
            "llm_load_vocab: control token:     30 '<unused23>' is not marked as EOG\n",
            "llm_load_vocab: control token:     62 '<unused55>' is not marked as EOG\n",
            "llm_load_vocab: control token:      3 '<unk>' is not marked as EOG\n",
            "llm_load_vocab: control token:     18 '<unused11>' is not marked as EOG\n",
            "llm_load_vocab: control token:     22 '<unused15>' is not marked as EOG\n",
            "llm_load_vocab: control token:     66 '<unused59>' is not marked as EOG\n",
            "llm_load_vocab: control token:     65 '<unused58>' is not marked as EOG\n",
            "llm_load_vocab: control token:     10 '<unused3>' is not marked as EOG\n",
            "llm_load_vocab: control token:    105 '<unused98>' is not marked as EOG\n",
            "llm_load_vocab: control token:     87 '<unused80>' is not marked as EOG\n",
            "llm_load_vocab: control token:    100 '<unused93>' is not marked as EOG\n",
            "llm_load_vocab: control token:     63 '<unused56>' is not marked as EOG\n",
            "llm_load_vocab: control token:     31 '<unused24>' is not marked as EOG\n",
            "llm_load_vocab: control token:     58 '<unused51>' is not marked as EOG\n",
            "llm_load_vocab: control token:     84 '<unused77>' is not marked as EOG\n",
            "llm_load_vocab: control token:     61 '<unused54>' is not marked as EOG\n",
            "llm_load_vocab: control token:      1 '<eos>' is not marked as EOG\n",
            "llm_load_vocab: control token:     60 '<unused53>' is not marked as EOG\n",
            "llm_load_vocab: control token:     91 '<unused84>' is not marked as EOG\n",
            "llm_load_vocab: control token:     83 '<unused76>' is not marked as EOG\n",
            "llm_load_vocab: control token:     85 '<unused78>' is not marked as EOG\n",
            "llm_load_vocab: control token:     27 '<unused20>' is not marked as EOG\n",
            "llm_load_vocab: control token:     96 '<unused89>' is not marked as EOG\n",
            "llm_load_vocab: control token:     72 '<unused65>' is not marked as EOG\n",
            "llm_load_vocab: control token:     53 '<unused46>' is not marked as EOG\n",
            "llm_load_vocab: control token:     82 '<unused75>' is not marked as EOG\n",
            "llm_load_vocab: control token:      7 '<unused0>' is not marked as EOG\n",
            "llm_load_vocab: control token:      4 '<mask>' is not marked as EOG\n",
            "llm_load_vocab: control token:    101 '<unused94>' is not marked as EOG\n",
            "llm_load_vocab: control token:     78 '<unused71>' is not marked as EOG\n",
            "llm_load_vocab: control token:     89 '<unused82>' is not marked as EOG\n",
            "llm_load_vocab: control token:     69 '<unused62>' is not marked as EOG\n",
            "llm_load_vocab: control token:     54 '<unused47>' is not marked as EOG\n",
            "llm_load_vocab: control token:     57 '<unused50>' is not marked as EOG\n",
            "llm_load_vocab: control token:     12 '<unused5>' is not marked as EOG\n",
            "llm_load_vocab: control token:     48 '<unused41>' is not marked as EOG\n",
            "llm_load_vocab: control token:     17 '<unused10>' is not marked as EOG\n",
            "llm_load_vocab: control token:     24 '<unused17>' is not marked as EOG\n",
            "llm_load_vocab: control token:     46 '<unused39>' is not marked as EOG\n",
            "llm_load_vocab: control token:     29 '<unused22>' is not marked as EOG\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 249\n",
            "llm_load_vocab: token to piece cache size = 1.6014 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = gemma2\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 256000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 2304\n",
            "llm_load_print_meta: n_layer          = 26\n",
            "llm_load_print_meta: n_head           = 8\n",
            "llm_load_print_meta: n_head_kv        = 4\n",
            "llm_load_print_meta: n_rot            = 256\n",
            "llm_load_print_meta: n_swa            = 4096\n",
            "llm_load_print_meta: n_embd_head_k    = 256\n",
            "llm_load_print_meta: n_embd_head_v    = 256\n",
            "llm_load_print_meta: n_gqa            = 2\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 9216\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 2B\n",
            "llm_load_print_meta: model ftype      = Q8_0\n",
            "llm_load_print_meta: model params     = 2.61 B\n",
            "llm_load_print_meta: model size       = 2.59 GiB (8.50 BPW) \n",
            "llm_load_print_meta: general.name     = Gemma 2 2b It\n",
            "llm_load_print_meta: BOS token        = 2 '<bos>'\n",
            "llm_load_print_meta: EOS token        = 1 '<eos>'\n",
            "llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n",
            "llm_load_print_meta: UNK token        = 3 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<pad>'\n",
            "llm_load_print_meta: LF token         = 227 '<0x0A>'\n",
            "llm_load_print_meta: EOG token        = 1 '<eos>'\n",
            "llm_load_print_meta: EOG token        = 107 '<end_of_turn>'\n",
            "llm_load_print_meta: max token length = 48\n",
            "llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "llm_load_tensors: offloading 26 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 27/27 layers to GPU\n",
            "llm_load_tensors:        CUDA0 model buffer size =  2649.78 MiB\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   597.66 MiB\n",
            ".................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 16000\n",
            "llama_new_context_with_model: n_ctx_per_seq = 16000\n",
            "llama_new_context_with_model: n_batch       = 512\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 10000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_pre_seq (16000) > n_ctx_train (8192) -- possible training context overflow\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1625.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1625.00 MiB, K (f16):  812.50 MiB, V (f16):  812.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.98 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   504.50 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    67.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1050\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.entries_count': '182', 'general.quantization_version': '2', 'quantize.imatrix.file': '/models_out/gemma-2-2b-it-GGUF/gemma-2-2b-it.imatrix', 'tokenizer.ggml.add_space_prefix': 'false', 'gemma2.attention.head_count': '8', 'gemma2.feed_forward_length': '9216', 'gemma2.block_count': '26', 'tokenizer.ggml.pre': 'default', 'general.license': 'gemma', 'general.type': 'model', 'gemma2.embedding_length': '2304', 'general.basename': 'gemma-2', 'tokenizer.ggml.padding_token_id': '0', 'gemma2.context_length': '8192', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'general.architecture': 'gemma2', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'gemma2.attention.key_length': '256', 'gemma2.attention.value_length': '256', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'general.finetune': 'it', 'general.file_type': '7', 'gemma2.attention.sliding_window': '4096', 'gemma2.attn_logit_softcapping': '50.000000', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.head_count_kv': '4', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.model': 'llama', 'general.name': 'Gemma 2 2b It', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'general.size_label': '2B', 'tokenizer.ggml.add_bos_token': 'true'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
            "' + message['content'] | trim + '<end_of_turn>\n",
            "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
            "'}}{% endif %}\n",
            "Using chat eos_token: <eos>\n",
            "Using chat bos_token: <bos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "On August 10, JSW Neo Energy has agreed to buy a portfolio of 1753 mega watt renewable energy generation capacity from Mytrah Energy India Pvt Ltd for Rs 10,530 crore.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "who is buying energy?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "## Generation kwargs\n",
        "generation_kwargs = {\n",
        "    \"max_tokens\":20000,\n",
        "    \"echo\":False, # Echo the prompt in the output\n",
        "    \"top_k\":1 # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value > 1 for sampling decoding\n",
        "}\n",
        "\n",
        "\n",
        "## Run inference\n",
        "res = llm(prompt=prompt, **generation_kwargs) # Res is a dictionary\n",
        "\n",
        "## Unpack and the generated text from the LLM response dictionary and print it\n",
        "print(res[\"choices\"][0][\"text\"])\n",
        "print(\"xxx\")\n",
        "# res is short for result\n",
        "\n",
        "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "On August 10, JSW Neo Energy has agreed to buy a portfolio of 1753 mega watt renewable energy generation capacity from Mytrah Energy India Pvt Ltd for Rs 10,530 crore.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "what is the deal?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "## Run inference\n",
        "res = llm(prompt=prompt, **generation_kwargs) # Res is a dictionary\n",
        "\n",
        "## Unpack and the generated text from the LLM response dictionary and print it\n",
        "print(res[\"choices\"][0][\"text\"])\n",
        "print(\"xxx\")\n",
        "\n",
        "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "On August 10, we said that JSW Neo Energy has agreed to buy a portfolio of 1753 mega watt renewable energy generation capacity from Mytrah Energy India Pvt Ltd for Rs 10,530 crore.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "what is the date of the announcement ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "## Run inference\n",
        "res = llm(prompt=prompt, **generation_kwargs) # Res is a dictionary\n",
        "\n",
        "## Unpack and the generated text from the LLM response dictionary and print it\n",
        "print(res[\"choices\"][0][\"text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKSTQPhhSJuD",
        "outputId": "6f64a256-f3f3-4f10-eec0-e03784d9c115"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 98 prefix-match hit, remaining 28 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =     315.87 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    28 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     392.11 ms /    43 tokens\n",
            "Llama.generate: 98 prefix-match hit, remaining 28 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "JSW Neo Energy is the company buying the energy portfolio. \n",
            "\n",
            "xxx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =     315.87 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    28 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /   359 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =    6492.20 ms /   387 tokens\n",
            "Llama.generate: 33 prefix-match hit, remaining 100 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "JSW Neo Energy, a subsidiary of JSW Group, has acquired a portfolio of 1753 MW of renewable energy generation capacity from Mytrah Energy India Pvt Ltd. This deal is valued at Rs 10,530 crore (approximately $1.3 billion).\n",
            "\n",
            "Here's a breakdown of the deal:\n",
            "\n",
            "* **Buyer:** JSW Neo Energy, a subsidiary of JSW Group, a leading Indian conglomerate.\n",
            "* **Seller:** Mytrah Energy India Pvt Ltd, a leading renewable energy developer in India.\n",
            "* **Assets:** A portfolio of 1753 MW of renewable energy generation capacity, including solar, wind, and hydro projects.\n",
            "* **Value:** Rs 10,530 crore (approximately $1.3 billion).\n",
            "\n",
            "**Significance of the deal:**\n",
            "\n",
            "* **Expansion of JSW Neo Energy's renewable energy portfolio:** This acquisition will significantly expand JSW Neo Energy's renewable energy portfolio, making it a major player in the Indian renewable energy market.\n",
            "* **Mytrah Energy's exit from the Indian market:** This deal marks Mytrah Energy's exit from the Indian renewable energy market, as they have been focusing on other regions.\n",
            "* **Investment in renewable energy:** This deal highlights the growing investment in renewable energy in India, as the country aims to achieve its ambitious renewable energy targets.\n",
            "\n",
            "**Key details:**\n",
            "\n",
            "* The deal is expected to be completed by the end of 2023.\n",
            "* The acquisition will be funded through a combination of debt and equity financing.\n",
            "* The transaction is subject to regulatory approvals.\n",
            "\n",
            "\n",
            "**Overall, this deal signifies a significant step forward in India's transition to a cleaner energy future.**\n",
            "\n",
            "xxx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =     315.87 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    41 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =     757.74 ms /   141 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The announcement date was August 10, 2023. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n"
          ]
        }
      ]
    }
  ]
}